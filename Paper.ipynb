{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import warnings\n",
    "import vl_convert as vlc\n",
    "import cairosvg\n",
    "warnings.filterwarnings('ignore')\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-33142485c6074c97890e941157f9ad8d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-33142485c6074c97890e941157f9ad8d.vega-embed details,\n",
       "  #altair-viz-33142485c6074c97890e941157f9ad8d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-33142485c6074c97890e941157f9ad8d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-33142485c6074c97890e941157f9ad8d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-33142485c6074c97890e941157f9ad8d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.8.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.8.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 12}, \"legend\": {\"labelFontSize\": 12, \"titleFontSize\": 12}}, \"data\": {\"name\": \"data-baa38797145d10eef1cab3c738b6604c\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"x\": {\"field\": \"since\", \"title\": \"Time since Start\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"latency\", \"title\": \"End-to-end lantency (ms)\", \"type\": \"quantitative\"}}, \"height\": 300, \"width\": 300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.8.0.json\", \"datasets\": {\"data-baa38797145d10eef1cab3c738b6604c\": [{\"latency\": 58.816705, \"since\": 1.0}, {\"latency\": 59.756902000000004, \"since\": 2.0}, {\"latency\": 59.026806, \"since\": 3.0}, {\"latency\": 59.940077, \"since\": 4.0}, {\"latency\": 59.647895, \"since\": 5.0}, {\"latency\": 59.1591455, \"since\": 6.0}, {\"latency\": 59.0997075, \"since\": 7.0}, {\"latency\": 58.977763499999995, \"since\": 8.0}, {\"latency\": 58.972851, \"since\": 9.0}, {\"latency\": 59.717345, \"since\": 10.0}, {\"latency\": 59.9326475, \"since\": 11.0}, {\"latency\": 60.277892, \"since\": 12.0}, {\"latency\": 59.62751, \"since\": 13.0}, {\"latency\": 58.90099, \"since\": 14.0}, {\"latency\": 60.1360185, \"since\": 15.0}, {\"latency\": 60.030513, \"since\": 16.0}, {\"latency\": 59.355625, \"since\": 17.0}, {\"latency\": 59.24483549999999, \"since\": 18.0}, {\"latency\": 58.397028000000006, \"since\": 19.0}, {\"latency\": 59.312558, \"since\": 20.0}, {\"latency\": 59.011091, \"since\": 21.0}, {\"latency\": 60.0671105, \"since\": 22.0}, {\"latency\": 59.584016, \"since\": 23.0}, {\"latency\": 59.51835, \"since\": 24.0}, {\"latency\": 59.644695, \"since\": 25.0}, {\"latency\": 59.412223, \"since\": 26.0}, {\"latency\": 59.240118499999994, \"since\": 27.0}, {\"latency\": 59.643179, \"since\": 28.0}, {\"latency\": 59.567729, \"since\": 29.0}, {\"latency\": 59.75946, \"since\": 30.0}, {\"latency\": 59.5498525, \"since\": 31.0}, {\"latency\": 59.04554, \"since\": 32.0}, {\"latency\": 58.581118499999995, \"since\": 33.0}, {\"latency\": 44.647197, \"since\": 34.0}, {\"latency\": 44.4589295, \"since\": 35.0}, {\"latency\": 44.535656, \"since\": 36.0}, {\"latency\": 44.433205, \"since\": 37.0}, {\"latency\": 44.2637305, \"since\": 38.0}, {\"latency\": 44.719815, \"since\": 39.0}, {\"latency\": 44.4747635, \"since\": 40.0}, {\"latency\": 44.540416, \"since\": 41.0}, {\"latency\": 44.465148000000006, \"since\": 42.0}, {\"latency\": 44.408957, \"since\": 43.0}, {\"latency\": 44.925182500000005, \"since\": 44.0}, {\"latency\": 49.045054, \"since\": 45.0}, {\"latency\": 44.501068999999994, \"since\": 46.0}, {\"latency\": 44.520098000000004, \"since\": 47.0}, {\"latency\": 44.422247999999996, \"since\": 48.0}, {\"latency\": 44.417708, \"since\": 49.0}, {\"latency\": 44.427085500000004, \"since\": 50.0}, {\"latency\": 44.421815, \"since\": 51.0}, {\"latency\": 44.458272, \"since\": 52.0}, {\"latency\": 45.5367725, \"since\": 53.0}, {\"latency\": 49.211158999999995, \"since\": 54.0}, {\"latency\": 49.289917, \"since\": 55.0}, {\"latency\": 49.206541, \"since\": 56.0}, {\"latency\": 49.3620215, \"since\": 57.0}, {\"latency\": 49.162400000000005, \"since\": 58.0}, {\"latency\": 58.819727, \"since\": 59.0}, {\"latency\": 58.787659999999995, \"since\": 60.0}, {\"latency\": 59.033045, \"since\": 61.0}, {\"latency\": 59.041312999999995, \"since\": 62.0}, {\"latency\": 47.561589999999995, \"since\": 63.0}, {\"latency\": 43.418118, \"since\": 64.0}, {\"latency\": 43.261635999999996, \"since\": 65.0}, {\"latency\": 42.911585, \"since\": 66.0}, {\"latency\": 42.62494, \"since\": 67.0}, {\"latency\": 42.501013, \"since\": 68.0}, {\"latency\": 42.6351735, \"since\": 69.0}, {\"latency\": 42.306158, \"since\": 70.0}, {\"latency\": 42.550966, \"since\": 71.0}, {\"latency\": 42.694176999999996, \"since\": 72.0}, {\"latency\": 42.585893, \"since\": 73.0}, {\"latency\": 42.458997, \"since\": 74.0}, {\"latency\": 42.834128, \"since\": 75.0}, {\"latency\": 42.630898, \"since\": 76.0}, {\"latency\": 42.916675999999995, \"since\": 77.0}, {\"latency\": 42.865137999999995, \"since\": 78.0}, {\"latency\": 43.045978999999996, \"since\": 79.0}, {\"latency\": 43.266224, \"since\": 80.0}, {\"latency\": 43.2903505, \"since\": 81.0}, {\"latency\": 43.237536999999996, \"since\": 82.0}, {\"latency\": 43.105571499999996, \"since\": 83.0}, {\"latency\": 43.442372, \"since\": 84.0}, {\"latency\": 43.385089, \"since\": 85.0}, {\"latency\": 43.185146, \"since\": 86.0}, {\"latency\": 42.997239, \"since\": 87.0}, {\"latency\": 42.9594155, \"since\": 88.0}, {\"latency\": 42.9285555, \"since\": 89.0}, {\"latency\": 42.980876, \"since\": 90.0}, {\"latency\": 43.8544145, \"since\": 91.0}, {\"latency\": 50.551178, \"since\": 92.0}, {\"latency\": 54.249696, \"since\": 93.0}, {\"latency\": 50.581899, \"since\": 94.0}, {\"latency\": 59.4050725, \"since\": 95.0}, {\"latency\": 60.064077, \"since\": 96.0}, {\"latency\": 58.897268000000004, \"since\": 97.0}, {\"latency\": 59.629206, \"since\": 98.0}, {\"latency\": 58.45212, \"since\": 99.0}, {\"latency\": 59.650641, \"since\": 100.0}, {\"latency\": 60.546884500000004, \"since\": 101.0}, {\"latency\": 60.480436999999995, \"since\": 102.0}, {\"latency\": 59.838661, \"since\": 103.0}, {\"latency\": 59.37926, \"since\": 104.0}, {\"latency\": 60.604546, \"since\": 105.0}, {\"latency\": 59.317846, \"since\": 106.0}, {\"latency\": 60.554093, \"since\": 107.0}, {\"latency\": 59.731539999999995, \"since\": 108.0}, {\"latency\": 59.971759, \"since\": 109.0}, {\"latency\": 59.7231115, \"since\": 110.0}, {\"latency\": 59.879121, \"since\": 111.0}, {\"latency\": 59.164419, \"since\": 112.0}, {\"latency\": 60.0596115, \"since\": 113.0}, {\"latency\": 58.998843, \"since\": 114.0}, {\"latency\": 59.477022999999996, \"since\": 115.0}, {\"latency\": 60.7367005, \"since\": 116.0}, {\"latency\": 60.380160000000004, \"since\": 117.0}, {\"latency\": 60.235477, \"since\": 118.0}, {\"latency\": 59.294270000000004, \"since\": 119.0}, {\"latency\": 59.2705705, \"since\": 120.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_results(prefix, expt_name, since=0):\n",
    "    results_file = f\"results/infer_bench/{prefix}_{expt_name}.csv\"\n",
    "    if expt_name.startswith(\"High\"):\n",
    "        usage = \"3. High\"\n",
    "    if expt_name == \"Medium\":\n",
    "        usage = \"2. Medium\"\n",
    "    if expt_name == \"Low\":\n",
    "        usage = \"1. Low\" if prefix == \"pre\" else \"4. Low\"\n",
    "    columns = [\"since\", \"latency\", \"exec_mode\"]\n",
    "    results = pd.read_csv(results_file, names=columns)\n",
    "    results[\"latency\"] = results.latency * 1000.0 # Convert to ms.\n",
    "    results[\"since\"] = results[\"since\"] / 1000.0 + since\n",
    "    results[\"Usage\"] = [usage for _ in range(0, len(results))]\n",
    "    p99 = results.latency.quantile(0.99)\n",
    "    results = results[results.latency < p99]\n",
    "    return results\n",
    "\n",
    "\n",
    "res1 = read_results(\"pre\", \"Low\", 0)\n",
    "low_avg = res1.latency.mean()\n",
    "res2 = read_results(\"pre\", \"Medium\", res1.since.max())\n",
    "medium_avg = res2.latency.mean()\n",
    "res3 = read_results(\"pre\", \"High(10)\", res2.since.max())\n",
    "high_avg = res3.latency.mean()\n",
    "pre_res = pd.concat([res1, res2, res3])\n",
    "res4 = read_results(\"post\", \"Low\", res3.since.max())\n",
    "res = pd.concat([res1, res2, res3, res4])\n",
    "\n",
    "\n",
    "# print(\"Latencies\")\n",
    "# chart1.display()\n",
    "\n",
    "# expt_dir = \"figures/inference\"\n",
    "# svg_str = vlc.vegalite_to_svg(chart1.to_json())\n",
    "# cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/latencies.pdf\")\n",
    "\n",
    "res[\"since\"] = pd.to_timedelta(res.since, unit='s')\n",
    "res = res[[\"since\", \"latency\"]].groupby(pd.Grouper(key=\"since\", freq=\"1min\")).median()\n",
    "res[\"since\"] = res.index.total_seconds() / 60 + 1\n",
    "\n",
    "chart1 = alt.Chart(res).mark_line().encode(\n",
    "    x = alt.X('since', title='Time since Start'),\n",
    "    y = alt.Y('latency', title='End-to-end lantency (ms)'),\n",
    ").properties(\n",
    "    # title = \"Read performance distribution\",\n",
    "    width=300,\n",
    "    height=300\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "chart1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg cost per month.\n",
    "def comparison_nums(avg_latency, median_latency, mode):\n",
    "    if mode == \"Low\":\n",
    "        reqs_per_sec = 1\n",
    "    if mode == \"Medium\":\n",
    "        reqs_per_sec = 50\n",
    "    if mode == \"High\":\n",
    "        reqs_per_sec = 200\n",
    "    # Reqs in an month.\n",
    "    num_hours = 24 * 30\n",
    "    total_reqs = 3600.0 * reqs_per_sec * num_hours\n",
    "    print(total_reqs)\n",
    "    pure_lambda_latency = low_median # Same for all other modes.\n",
    "    print(pure_lambda_latency)\n",
    "    pure_lambda_cost = total_reqs * ((low_avg - 15.0) * 4 * 0.0000000167 + 0.2 / 1e6) # 4GB of mem. \n",
    "    pure_lambda_user_cost = total_reqs * low_avg * 0.5 * 0.0000000167\n",
    "    visc_latency = median_latency\n",
    "    if mode == \"Low\":\n",
    "        visc_cost = pure_lambda_cost\n",
    "    if mode == \"Medium\":\n",
    "        visc_cost = num_hours * 0.015 * 2.0 * 2.0 # 2vcpus (x2 for excess sizing).\n",
    "    if mode == \"High\":\n",
    "        visc_cost  = num_hours * 9 * (0.015 * 2.0 * 2.0) # Like above, but with 3 instances.\n",
    "    visc_user_cost = total_reqs * avg_latency * 0.5 * 0.0000000167\n",
    "    \n",
    "    systems = []\n",
    "    modes = []\n",
    "    latencies = []\n",
    "    compute_costs = []\n",
    "    overall_costs = []\n",
    "    if mode == \"Low\":\n",
    "        mode = \"1. Low\"\n",
    "    if mode == \"Medium\":\n",
    "        mode = \"2. Medium\"\n",
    "    if mode == \"High\":\n",
    "        mode = \"3. High\"\n",
    "    systems.append(\"Pure Lambda\")\n",
    "    modes.append(mode)\n",
    "    latencies.append(pure_lambda_latency)\n",
    "    compute_costs.append(pure_lambda_cost)\n",
    "    overall_costs.append(pure_lambda_cost + pure_lambda_user_cost)\n",
    "    systems.append(\"VISC Inference\")\n",
    "    modes.append(mode)\n",
    "    latencies.append(visc_latency)\n",
    "    compute_costs.append(visc_cost)\n",
    "    overall_costs.append(visc_cost + visc_user_cost)\n",
    "    return pd.DataFrame({\n",
    "        \"systems\": systems,\n",
    "        \"modes\": modes,\n",
    "        \"latencies\": latencies,\n",
    "        \"compute_costs\": compute_costs,\n",
    "        \"overall_costs\": overall_costs,\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "low_median = res1.latency.quantile(0.5)\n",
    "medium_median = res2.latency.quantile(0.5)\n",
    "high_median = res3.latency.quantile(0.5)\n",
    "cres1 = comparison_nums(low_avg, low_median, \"Low\")\n",
    "cres2 = comparison_nums(medium_avg, medium_median, \"Medium\")\n",
    "cres3 = comparison_nums(high_avg, high_median, \"High\")\n",
    "cres = pd.concat([cres1, cres2, cres3])\n",
    "\n",
    "chart1 = alt.Chart(data=cres).mark_bar().encode(\n",
    "    x=alt.X(\"systems\", title=\"\"),\n",
    "    y=alt.Y(\"compute_costs\", title=\"Monthly Cost\"),\n",
    "    column=alt.Column(\"modes\", title=\"Utilization\"),\n",
    "    color=alt.Color(\"systems\", legend=None)\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=100,\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "chart2 = alt.Chart(data=cres).mark_bar().encode(\n",
    "    x=alt.X(\"systems\", title=\"\"),\n",
    "    y=alt.Y(\"overall_costs\", title=\"Monthly Cost\"),\n",
    "    column=alt.Column(\"modes\", title=\"Utilization\"),\n",
    "    color=alt.Color(\"systems\", legend=None)\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=100,\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "chart3 = alt.Chart(data=cres).mark_bar().encode(\n",
    "    x=alt.X(\"systems\", title=\"\"),\n",
    "    y=alt.Y(\"latencies\", title=\"Latency\"),\n",
    "    column=alt.Column(\"modes\", title=\"Utilization\"),\n",
    "    color=alt.Color(\"systems\", legend=None)\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=100,\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "print(\"Compute Costs\")\n",
    "chart1.display()\n",
    "print(\"Overall Costs\")\n",
    "chart2.display()\n",
    "print(\"Latencies\")\n",
    "chart3.display()\n",
    "\n",
    "expt_dir = \"figures/inference\"\n",
    "svg_str = vlc.vegalite_to_svg(chart1.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/compute_cost_comparison.pdf\")\n",
    "svg_str = vlc.vegalite_to_svg(chart2.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/overall_cost_comparison.pdf\")\n",
    "svg_str = vlc.vegalite_to_svg(chart3.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/latency_comparison.pdf\")\n",
    "\n",
    "cres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "52.790602 * (1 + 0.1585189538092405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(prefix, expt_name, since=0):\n",
    "    results_file = f\"results/microactor_bench/{prefix}_{expt_name}.csv\"\n",
    "    if expt_name.startswith(\"High\"):\n",
    "        usage = \"3. High\"\n",
    "    if expt_name == \"Medium\":\n",
    "        usage = \"2. Medium\"\n",
    "    if expt_name == \"Low\":\n",
    "        usage = \"1. Low\" if prefix == \"pre\" else \"4. Low\"\n",
    "    columns = [\"since\", \"latency\", \"operation\"]\n",
    "    results = pd.read_csv(results_file, names=columns)\n",
    "    results[\"latency\"] = results.latency * 1000.0 # Convert to ms.\n",
    "    results[\"since\"] = results[\"since\"] / 1000.0 + since\n",
    "    results[\"Usage\"] = [usage for _ in range(0, len(results))]\n",
    "    p99 = results.latency.quantile(0.99)\n",
    "    results = results[results.latency < p99]\n",
    "    return results\n",
    "\n",
    "# Compute results.\n",
    "res1 = read_results(\"pre\", \"Low\", 0)\n",
    "low_avg = 2 * res1.latency.mean() # One retrieve and one increment request.\n",
    "res2 = read_results(\"pre\", \"Medium\", res1.since.max())\n",
    "medium_avg = 2 * res2.latency.mean()\n",
    "res3 = read_results(\"pre\", \"High(10)\", res2.since.max())\n",
    "high_avg = 2 * res3.latency.mean()\n",
    "pre_res = pd.concat([res1, res2, res3])\n",
    "res4 = read_results(\"post\", \"Low\", res3.since.max())\n",
    "res = pd.concat([res1, res2, res3, res4])\n",
    "\n",
    "chart1 = alt.Chart(res[res.operation == \"Retrieve\"]).mark_boxplot(extent=\"min-max\", size=50).encode(\n",
    "    x = alt.X('Usage', title='Utilization'),\n",
    "    y = alt.Y('latency', scale=alt.Scale(type=\"symlog\"), axis=alt.Axis(values=[0, 1, 10, 100, 1000]), title='End-to-end lantency (ms)'),\n",
    ").properties(\n",
    "    # title = \"Read performance distribution\",\n",
    "    width=300,\n",
    "    height=300\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "chart2 = alt.Chart(res[res.operation == \"Increment\"]).mark_boxplot(extent=\"min-max\", size=50).encode(\n",
    "    x = alt.X('Usage', title='Utilization'),\n",
    "    y = alt.Y('latency', scale=alt.Scale(type=\"symlog\"), axis=alt.Axis(values=[0, 1, 10, 100, 1000]), title='End-to-end lantency (ms)'),\n",
    ").properties(\n",
    "    # title = \"Read performance distribution\",\n",
    "    width=300,\n",
    "    height=300\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "print(\"Retrieve Latencies\")\n",
    "chart1.display()\n",
    "print(\"Increment Latencies\")\n",
    "chart2.display()\n",
    "\n",
    "\n",
    "expt_dir = \"figures/actor/\"\n",
    "svg_str = vlc.vegalite_to_svg(chart1.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/retrieve_latencies.pdf\")\n",
    "svg_str = vlc.vegalite_to_svg(chart2.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/increment_latencies.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Avg cost per month.\n",
    "def comparison_nums(avg_latency, median_latency, mode):\n",
    "    if mode == \"Low\":\n",
    "        reqs_per_sec = 1\n",
    "    if mode == \"Medium\":\n",
    "        reqs_per_sec = 100\n",
    "    if mode == \"High\":\n",
    "        reqs_per_sec = 1000\n",
    "    # Reqs in an month.\n",
    "    num_hours = 24 * 30\n",
    "    total_reqs = 3600.0 * reqs_per_sec * num_hours\n",
    "    print(total_reqs)\n",
    "    pure_lambda_latency = low_median # Same for all other modes.\n",
    "    print(pure_lambda_latency)\n",
    "    pure_lambda_cost = total_reqs * ((low_avg - 10.0) * 4 * 0.0000000167 + 0.2 / 1e6) # 4GB of mem.\n",
    "    pure_lambda_user_cost = total_reqs * low_avg * 0.5 * 0.0000000167\n",
    "    pure_ecs_latency = medium_median\n",
    "    pure_ecs_cost = num_hours * 0.015 * 2.0 # 2vcpus.\n",
    "    pure_ecs_user_cost = total_reqs * medium_avg * 0.5 * 0.0000000167\n",
    "    visc_latency = median_latency\n",
    "    if mode == \"Low\":\n",
    "        visc_cost = pure_lambda_cost\n",
    "    if mode == \"Medium\":\n",
    "        visc_cost = pure_ecs_cost\n",
    "    if mode == \"High\":\n",
    "        visc_cost  = num_hours * 0.015 * (2.0 + 6*0.25) # 2vcpus and 6 times 0.25vcpus.\n",
    "    visc_user_cost = total_reqs * avg_latency * 0.5 * 0.0000000167\n",
    "    \n",
    "    systems = []\n",
    "    modes = []\n",
    "    latencies = []\n",
    "    compute_costs = []\n",
    "    overall_costs = []\n",
    "    if mode == \"Low\":\n",
    "        mode = \"1. Low\"\n",
    "    if mode == \"Medium\":\n",
    "        mode = \"2. Medium\"\n",
    "    if mode == \"High\":\n",
    "        mode = \"3. High\"\n",
    "    if \"High\" not in mode:\n",
    "        systems.append(\"Lambda\")\n",
    "        modes.append(mode)\n",
    "        latencies.append(pure_lambda_latency)\n",
    "        compute_costs.append(pure_lambda_cost)\n",
    "        overall_costs.append(pure_lambda_cost + pure_lambda_user_cost)\n",
    "    systems.append(\"Container\")\n",
    "    modes.append(mode)\n",
    "    latencies.append(pure_ecs_latency)\n",
    "    compute_costs.append(pure_ecs_cost)\n",
    "    overall_costs.append(pure_ecs_cost + pure_ecs_user_cost)\n",
    "    systems.append(\"VISC\")\n",
    "    modes.append(mode)\n",
    "    latencies.append(visc_latency)\n",
    "    compute_costs.append(visc_cost)\n",
    "    overall_costs.append(visc_cost + visc_user_cost)\n",
    "    return pd.DataFrame({\n",
    "        \"systems\": systems,\n",
    "        \"modes\": modes,\n",
    "        \"latencies\": latencies,\n",
    "        \"compute_costs\": compute_costs,\n",
    "        \"overall_costs\": overall_costs,\n",
    "    })\n",
    "\n",
    "# Compute medians.\n",
    "low_median = res1[res1[\"operation\"] == \"Retrieve\"].latency.quantile(0.5) + res1[res1[\"operation\"] == \"Increment\"].latency.quantile(0.5)\n",
    "medium_median = res2[res2[\"operation\"] == \"Retrieve\"].latency.quantile(0.5) + res2[res2[\"operation\"] == \"Increment\"].latency.quantile(0.5)\n",
    "high_median = res3[res3[\"operation\"] == \"Retrieve\"].latency.quantile(0.5) + res3[res3[\"operation\"] == \"Increment\"].latency.quantile(0.5)\n",
    "cres1 = comparison_nums(low_avg, low_median, \"Low\")\n",
    "cres2 = comparison_nums(medium_avg, medium_median, \"Medium\")\n",
    "cres3 = comparison_nums(high_avg, high_median, \"High\")\n",
    "cres = pd.concat([cres1, cres2, cres3])\n",
    "\n",
    "chart1 = alt.Chart(data=cres).mark_bar().encode(\n",
    "    x=alt.X(\"systems\", title=\"\"),\n",
    "    y=alt.Y(\"compute_costs\", title=\"Monthly Cost\"),\n",
    "    column=alt.Column(\"modes\", title=\"Utilization\"),\n",
    "    color=alt.Color(\"systems\", legend=None)\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=100,\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "chart2 = alt.Chart(data=cres).mark_bar().encode(\n",
    "    x=alt.X(\"systems\", title=\"\"),\n",
    "    y=alt.Y(\"overall_costs\", title=\"Monthly Cost\"),\n",
    "    column=alt.Column(\"modes\", title=\"Utilization\"),\n",
    "    color=alt.Color(\"systems\", legend=None)\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=100,\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "chart3 = alt.Chart(data=cres).mark_bar().encode(\n",
    "    x=alt.X(\"systems\", title=\"\"),\n",
    "    y=alt.Y(\"latencies\", title=\"Latency\"),\n",
    "    column=alt.Column(\"modes\", title=\"Utilization\"),\n",
    "    color=alt.Color(\"systems\", legend=None)\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=100,\n",
    ").configure_axis(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12\n",
    ").configure_legend(\n",
    "    titleFontSize=12,\n",
    "    labelFontSize=12,\n",
    ")\n",
    "\n",
    "print(\"Compute Costs\")\n",
    "chart1.display()\n",
    "print(\"Overall Costs\")\n",
    "chart2.display()\n",
    "print(\"Latencies\")\n",
    "chart3.display()\n",
    "\n",
    "expt_dir = \"figures/actor\"\n",
    "svg_str = vlc.vegalite_to_svg(chart1.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/compute_cost_comparison.pdf\")\n",
    "svg_str = vlc.vegalite_to_svg(chart2.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/overall_cost_comparison.pdf\")\n",
    "svg_str = vlc.vegalite_to_svg(chart3.to_json())\n",
    "cairosvg.svg2pdf(bytestring=svg_str, write_to=f\"{expt_dir}/latency_comparison.pdf\")\n",
    "\n",
    "cres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "260.138862/123.920105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8 / 2.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
